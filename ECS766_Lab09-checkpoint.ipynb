{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab session 9: Data Mining Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "\n",
    "The aim of this lab is for students to get experience with data mining applications, in particular on **Text Mining** and **Timeseries Mining** methods covered in week 10.\n",
    "\n",
    "- This lab is the second part of a **two-week assignment** that covers weeks 9 and 10.\n",
    "- This lab corresponds to **Assignment 4** which is due on **Tuesday 8th December at 10am**, accounting for 10% of your overall grade. Questions in this lab sheet will contribute to 5% of your overall grade; questions in the lab sheet for week 9 will cover for another 5% of your overall grade.\n",
    "- <font color = 'maroon'>The last section of this notebook includes the questions that are assessed towards your final grade.</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important notes about the assignment: \n",
    "\n",
    "- **PLAGIARISM** <ins>is an irreversible non-negotiable failure in the course</ins> (if in doubt of what constitutes plagiarism, ask!). \n",
    "- The total assessed coursework is worth 40% of your final grade.\n",
    "- There will be 9 lab sessions and 4 assignments.\n",
    "- One assignment will cover 2 consecutive lab sessions and will be worth 10 marks (percentages of your final grade).\n",
    "- The submission cut-off date will be 7 days after the deadline and penalties will be applied for late submissions in accordance with the School policy on late submissions.\n",
    "- You are asked to submit a **report** that should answer the questions specified in the last section of this notebook. The report should be in **PDF format** (so **NOT** *doc, docx, notebook* etc). It should be well identified with your name, student number, assignment number (for instance, Assignment 4), module, and marked with question numbers. \n",
    "- No other means of submission other than submitting your assignment through the appropriate QM+ link are acceptable at any time. Submissions sent via email will **not** be considered.\n",
    "- Please name your report as follows: Assignment4-StudentName-StudentNumber.pdf\n",
    "- Cases of **Extenuating Circumstances (ECs)** have to go through the proper procedure of the School in due time. Only cases approved by the School in due time can be considered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Text Mining\n",
    "\n",
    "This first section on text mining will demonstrate a simple method for clustering documents using k-means. As our dataset, we will use articles mined from Wikipedia.\n",
    "\n",
    "Using python's [wikipedia package](https://pypi.org/project/wikipedia/), we can easily download content from Wikipedia and store it as a python object. For this example, we will use the content of the wikipedia entries that corredpond to the following terms:\n",
    "\n",
    "    Data Science\n",
    "    Artificial intelligence\n",
    "    Bank\n",
    "    Financial technology\n",
    "    Basketball\n",
    "    Swimming\n",
    "    Tennis\n",
    "\n",
    "The content of each Wikipedia article is stored in list 'wiki_list' while the title of each article will be stored in variable 'title'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wikipedia\n",
      "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\vaibhav\\anaconda3\\lib\\site-packages (from wikipedia) (4.6.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in c:\\users\\vaibhav\\anaconda3\\lib\\site-packages (from wikipedia) (2.25.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\vaibhav\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.22)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\vaibhav\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vaibhav\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2017.7.27.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\vaibhav\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.6)\n",
      "Building wheels for collected packages: wikipedia\n",
      "  Building wheel for wikipedia (setup.py): started\n",
      "  Building wheel for wikipedia (setup.py): finished with status 'done'\n",
      "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=13519 sha256=3f612ecd899fdf8781f12fa7aff31c969a282c13b2f03f749b32544ac6c0cad7\n",
      "  Stored in directory: c:\\users\\vaibhav\\appdata\\local\\pip\\cache\\wheels\\e4\\26\\c3\\4f401d3e5c907d26a2c4ca8484141f02f7630c98fbcfd78a5e\n",
      "Successfully built wikipedia\n",
      "Installing collected packages: wikipedia\n",
      "Successfully installed wikipedia-1.4.0\n",
      "Collecting statsmodels\n",
      "  Downloading statsmodels-0.12.1-cp36-none-win_amd64.whl (9.1 MB)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.15 in c:\\users\\vaibhav\\anaconda3\\lib\\site-packages (from statsmodels) (1.18.0)\n",
      "Collecting pandas>=0.21\n",
      "  Downloading pandas-1.1.4-cp36-cp36m-win_amd64.whl (8.7 MB)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=1.1 in c:\\users\\vaibhav\\anaconda3\\lib\\site-packages (from statsmodels) (1.1.0)\n",
      "Collecting patsy>=0.5\n",
      "  Downloading patsy-0.5.1-py2.py3-none-any.whl (231 kB)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in c:\\users\\vaibhav\\anaconda3\\lib\\site-packages (from pandas>=0.21->statsmodels) (2017.2)\n",
      "Collecting python-dateutil>=2.7.3\n",
      "  Downloading python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB)\n",
      "Requirement already satisfied, skipping upgrade: six in c:\\users\\vaibhav\\anaconda3\\lib\\site-packages (from patsy>=0.5->statsmodels) (1.15.0)\n",
      "Installing collected packages: python-dateutil, pandas, patsy, statsmodels\n",
      "  Attempting uninstall: python-dateutil\n",
      "    Found existing installation: python-dateutil 2.6.1\n",
      "    Uninstalling python-dateutil-2.6.1:\n",
      "      Successfully uninstalled python-dateutil-2.6.1\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 0.20.3\n",
      "    Uninstalling pandas-0.20.3:\n",
      "      Successfully uninstalled pandas-0.20.3\n",
      "  Attempting uninstall: patsy\n",
      "    Found existing installation: patsy 0.4.1\n",
      "    Uninstalling patsy-0.4.1:\n",
      "      Successfully uninstalled patsy-0.4.1\n",
      "  Attempting uninstall: statsmodels\n",
      "    Found existing installation: statsmodels 0.8.0\n",
      "    Uninstalling statsmodels-0.8.0:\n",
      "      Successfully uninstalled statsmodels-0.8.0\n",
      "Successfully installed pandas-1.1.4 patsy-0.5.1 python-dateutil-2.8.1 statsmodels-0.12.1\n"
     ]
    }
   ],
   "source": [
    "# Install/upgrade wikipedia and statsmodels packages for the lab\n",
    "!pip install wikipedia\n",
    "!pip install statsmodels --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading content:  Data Science\n",
      "loading content:  Artificial intelligence\n",
      "loading content:  Bank\n",
      "loading content:  Financial technology\n",
      "loading content:  Basketball\n",
      "loading content:  Swimming\n",
      "loading content:  Tennis\n"
     ]
    },
    {
     "ename": "PageError",
     "evalue": "Page id \"tenic\" does not match any pages. Try another id!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPageError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-de5aea767dbc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0marticle\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marticles\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m    \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"loading content: \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0marticle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m    \u001b[0mwiki_lst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwikipedia\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marticle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m    \u001b[0mtitle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marticle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py\u001b[0m in \u001b[0;36mpage\u001b[1;34m(title, pageid, auto_suggest, redirect, preload)\u001b[0m\n\u001b[0;32m    274\u001b[0m         \u001b[1;31m# if there is no suggestion or search results, the page doesn't exist\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mPageError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 276\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mWikipediaPage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mredirect\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mredirect\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreload\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpreload\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    277\u001b[0m   \u001b[1;32melif\u001b[0m \u001b[0mpageid\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mWikipediaPage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpageid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpageid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreload\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpreload\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, title, pageid, redirect, preload, original_title)\u001b[0m\n\u001b[0;32m    297\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Either a title or a pageid must be specified\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 299\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mredirect\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mredirect\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreload\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpreload\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    300\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mpreload\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self, redirect, preload)\u001b[0m\n\u001b[0;32m    343\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;34m'missing'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpage\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'title'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mPageError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    346\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mPageError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpageid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpageid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPageError\u001b[0m: Page id \"tenic\" does not match any pages. Try another id!"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import wikipedia\n",
    "\n",
    "articles=['Data Science','Artificial intelligence','Bank','Financial technology','Basketball','Swimming','Tennis']\n",
    "wiki_lst=[]\n",
    "title=[]\n",
    "\n",
    "# Load wikipedia articles\n",
    "for article in articles:\n",
    "   print(\"loading content: \",article)\n",
    "   wiki_lst.append(wikipedia.page(article).content)\n",
    "   title.append(article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, we can display the first retrieved entry, which was for field \"Data Science\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Printing the first retrieved entry\n",
    "print(wiki_lst[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are going to use the k-means clustering algorithm, we need to first represent each article using a vector space representation mentioned in the lectures. \n",
    "\n",
    "One popular vector space representation used in document clustering and other natural language processing tasks is the so-called **term-frequency/inverse-document-frequency (tf-idf)**. With this representation, for each word $w$ and document $d$ we calculate:\n",
    "\n",
    "Term frequency $\\mathit{tf}(w,d)$: the ratio of the number of appearances of word $w$ in document $d$ divided with the total number of words in document $d$.\n",
    "\n",
    "Inverse document frequency $\\mathit{idf}(w)$: the logarithm of the fraction of the total number of documents divided by the number of documents that contain word $w$.\n",
    "\n",
    "The tf-idf feature is then defined as:\n",
    "\\begin{equation}\n",
    "\\mathit{tfidf}(w,d)=\\mathit{tf}(w,d) \\cdot \\mathit{idf}(w)\n",
    "\\end{equation}\n",
    "\n",
    "For computing the above feature, it is recommended that stop words are excluded. All the calculations are easily done with sklearn’s [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words={'english'})\n",
    "X = vectorizer.fit_transform(wiki_lst) # Create tf-idf feature of the wikipedia dataset\n",
    "\n",
    "print(X.shape) # Print dimensions of tf-idf feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, each row of variable X is a feature representation of a Wikipedia article.\n",
    "\n",
    "Now we have converted our dataset in a representation suitable for clustering and other data mining operations, and X can be used as input for the k-means algorithm.\n",
    "\n",
    "One issue with the k-means algorithm is that the user needs to specify the number of clusters. One way of estimating an appropriate number of clusters is using the so-called elbow method, which calculates the sum of squared distances for various values of k (here we will use values of k from 2 to 7)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "Sum_of_squared_distances = []\n",
    "K = range(2,7)\n",
    "\n",
    "for k in K:\n",
    "    km = KMeans(n_clusters=k, max_iter=200, n_init=10)\n",
    "    km = km.fit(X)\n",
    "    Sum_of_squared_distances.append(km.inertia_)\n",
    "\n",
    "plt.plot(K, Sum_of_squared_distances, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Sum_of_squared_distances')\n",
    "plt.title('Elbow Method')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot is almost a straight line, probably because we have too few articles. But at a closer examination a small dent appears for k=4. We will therefore try to cluster into 4 groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fit k-means model with k=4\n",
    "true_k = 4\n",
    "model = KMeans(n_clusters=true_k, init='k-means++', max_iter=200, n_init=10)\n",
    "model.fit(X)\n",
    "\n",
    "# Print list of documents and associated clusters\n",
    "labels=model.labels_\n",
    "wiki_cl=pd.DataFrame(list(zip(title,labels)),columns=['title','cluster'])\n",
    "print(wiki_cl.sort_values(by=['cluster']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the clustering produced by k-means is not perfect; some decisions are sensible (e.g. Basketball and Tennis are in the same cluster; Bank and Financial Technology are in the same cluster), some less so (e.g. Data Science and AI are not in the same cluster; AI belongs to the same cluster with Bank and Financial Technology). However, with more data, more advanced clustering methods, and more elaborate feature extraction and pre-processing, we might be able to mine useful information from our text data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mining timeseries data - Smoothing\n",
    "\n",
    "\n",
    "Smoothing is a technique applied to a timeseries to remove the fine-grained variation between time steps. The aim of smoothing is to remove noise and better expose the the underlying processes that generated the timeseries. Moving averages are a simple and common type of smoothing used in timeseries analysis and timeseries forecasting.\n",
    "\n",
    "Calculating a moving average involves creating a new series where the values comprise the average of raw observations in the original timeseries. A moving average requires that you specify a **window size**. This defines the number of raw observations used to calculate the moving average value. The “moving” part in the moving average refers to the fact that the window defined by its window width is slid along the timeseries to calculate the average values in the new series.\n",
    "\n",
    "For this lab, we will be looking at a particular case of moving average smoothing, called the **Trailing Moving Average**. In trailing moving average smoothing, the value at time $t$ is calculated as the average of the raw observations at and before time $t$.\n",
    "\n",
    "It should be noted that calculating a moving average of a timeseries makes some assumptions about your data. One of these assumptions is that our timeseries is stationary, or does not show obvious trends (such as long-term increasing or decreasing movements) or seasonality (consistent periodic structure).\n",
    "\n",
    "For this section, we will be using a dataset that describes the number of daily female births in California in 1959. The dataset units are a count and there are 365 observations corresponding to days in 1959. This dataset is a good example for exploring the moving average method as it does not show any clear trend or seasonality.\n",
    "\n",
    "First we load and plot the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "series = read_csv('births.csv', header=0, index_col=0)\n",
    "\n",
    "print(series.head())\n",
    "series.plot(figsize=(15,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving average can be used as a data preparation technique to create a smoothed version of the original dataset.\n",
    "\n",
    "The rolling() found in Pandas will automatically group observations into a window. We can specify the window size, and by default a trailing window is created. Once the window is created, we can take the mean value, and this is our transformed dataset. New observations in the future can be just as easily transformed by keeping the raw values for the last few observations and updating a new average value.\n",
    "\n",
    "To make this concrete, using trailing moving average with a window size of 3, the transformed value at time $t$ is calculated as the mean value for the previous 3 observations $\\{t-2, t-1, t\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Perform trailing moving average smoothing\n",
    "rolling = series.rolling(window=3) # using a window of 3 samples: t, t-1, t-2\n",
    "rolling_mean = rolling.mean()\n",
    "print(rolling_mean.head(10))\n",
    "\n",
    "# plot original and transformed dataset\n",
    "series.plot(figsize=(15,4),title='original timeseries')\n",
    "rolling_mean.plot(color='red', figsize=(15,4),title='smoothed timeseries')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the printed first few values of the smoothed dataset, we can see that the first 2 smoothed observations will need to be discarded.\n",
    "\n",
    "Please go through the pandas documentation for the Pandas [rolling() function](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.rolling.html) as to understand its input arguments and expected output. Following this, you can experiment with different window sizes and study the effect that different window sizes have on smoothing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Mining timeseries data - Forecasting\n",
    "\n",
    "Timeseries data is common in many domains, such as sensor networking, healthcare, and financial markets. In particular, **forecasting** is an important problem in timeseries analysis because it can be used to make predictions about data points in the future.\n",
    "\n",
    "Altohugh state-of-the-art methods for timeseries forecasting rely on machine learning methods, before exploring such methods it is a good idea to exhaust \"classical\" linear methods for timeseries forecaseing. Classical time series forecasting methods are focused on exploring linear relationships from timeseries data, and perform well on a wide range of problems, assuming that our data is suitably prepared and the method is well configured. This section will outline a series of classical methods for timeseries forecasting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Autoregressive (AR) Models\n",
    "\n",
    "As a first step, we will create a simple synthetic signal which we will use as our dataset. The signal is a simple linear function consisting of 100 samples, which is perturbed by random noise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialise\n",
    "from random import random\n",
    "import numpy as np\n",
    "\n",
    "# Create and plot a synthetic signal\n",
    "data = [x + random() for x in range(1, 100)]\n",
    "\n",
    "data_array = np.array(data)\n",
    "\n",
    "plt.plot(data_array)\n",
    "plt.title('Synthetic Dataset 1')\n",
    "plt.xlabel('Sample index')\n",
    "plt.ylabel('Sample value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Autoregression (AR)** is a timeseries modeling and forecasting method which models the next step in a sequence as a linear function of the observations at previous time steps. This method is suitable for univariate timeseries without trend and seasonal components.\n",
    "\n",
    "The notation for the model involves specifying the order of the model $p$ as a parameter to the AR function, e.g. AR(p). For example, AR(1) is a first-order autoregressive model, which uses information from the immediately previous value of the signal to make a prediction for the next value.\n",
    "\n",
    "More generally, for a signal $y$, an AR model defines the value $y_t$ (where $t$ is the time index) as a linear combination of the previous values of $y$ as follows:\n",
    "\\begin{equation}\n",
    "y_t = \\sum_{i=1}^{p} \\alpha_i \\cdot y_{t-i} + c + \\epsilon_t\n",
    "\\end{equation}\n",
    "where $p$ defines the model order, coefficients $\\alpha_i$ and $c$ are learned from training data, and $\\epsilon_t$ values are assumed to be white noise error terms that are uncorrelated with one another.\n",
    "\n",
    "We can now initialise, fit, and make a prediction for an AR model in order to predict the next value in the timeseries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialise\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "\n",
    "# Fit Autoregressive model\n",
    "model = AutoReg(data, lags=1,old_names=False) # \"lags\" indicates the model order\n",
    "model_fit = model.fit()\n",
    "\n",
    "# Make prediction\n",
    "yhat = model_fit.predict(len(data), len(data)+3) # arguments denote which dataset indices to predict\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many more input arguments that we can use in the AutoReg function; for more information on input/output arguments please view the relevant [AutoReg function documentation](https://www.statsmodels.org/stable/generated/statsmodels.tsa.ar_model.AutoReg.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Moving Average (MA) Models\n",
    "\n",
    "While autoregression is a useful predictive property of a timeseries, it does not always explain all the variations. In fact, the unexpected component of the variations (shocks), does impact future values of the timeseries. This component can be captured with the use of a **moving average model (MA)**. The moving average model predicts subsequent series values on the basis of the past history of deviations from predicted values. A deviation from a predicted value can be viewed as white noise, or a shock.\n",
    "\n",
    "The moving average model is defined as follows:\n",
    "\\begin{equation}\n",
    "y_t = \\sum_{i=1}^{q} b_i \\cdot \\epsilon_{t-i} + c + \\epsilon_t\n",
    "\\end{equation}\n",
    "Here, $c$ is the mean of the timeseries. The values $b_1,\\ldots,b_q$ are the coefficients that need to be learned from the data. The moving average model is quite different from the autoregressive model, in that it relates the current value to the mean of the series and the previous history of deviations from forecasts, rather than the actual values. Here the values of $\\epsilon_t$ are assumed to be white noise error terms that are uncorrelated with one another.\n",
    "\n",
    "We can now initialise, fit, and make a prediction for an MA model in order to predict the next value in the timeseries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialise\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from random import random\n",
    "\n",
    "# Fit MA model\n",
    "model = ARIMA(data, order=(0, 0, 3)) # The 3rd 'order' argument denotes q=3, ie. a 1st order MA model\n",
    "model_fit = model.fit()\n",
    "\n",
    "# Make prediction\n",
    "yhat = model_fit.predict(len(data), len(data)+3) # arguments denote which dataset indices to predict\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from the above ARIMA() function, this is a more general function that goes beyond an MA model. By setting the first two arguments for 'order' to 0, we can achieve an MA function. There are many more input arguments that we can use in the ARIMA function; for more information on input/output arguments please view the relevant [ARIMA function documentation](https://www.statsmodels.org/stable/generated/statsmodels.tsa.arima.model.ARIMA.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Autoregressive Moving Average (ARMA) Models\n",
    "\n",
    "A more general model may be obtained by combining the power of both the autoregressive model and the moving average model. The idea is to learn the appropriate impact of both the autocorrelations and the shocks in predicting timeseries values. The two models can be combined with $p$ autoregressive terms and $q$ moving average terms.\n",
    "\n",
    "This model is referred to as the ARMA model, which can be formulated as follows:\n",
    "\\begin{equation}\n",
    "y_t = \\sum_{i=1}^{p} \\alpha_i \\cdot y_{t-i} + \\sum_{i=1}^{q} b_i \\cdot \\epsilon_{t-i} + c + \\epsilon_t\n",
    "\\end{equation}\n",
    "The aforementioned model is the ARMA($p$, $q$) model.\n",
    "\n",
    "We will now create a new synthetic dataset of 100 samples which consists of (pseudo)random noise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create and plot a synthetic signal\n",
    "data2 = [random() for x in range(1, 100)]\n",
    "\n",
    "data_array_2 = np.array(data2)\n",
    "\n",
    "plt.plot(data_array_2)\n",
    "plt.title('Synthetic Dataset 2')\n",
    "plt.xlabel('Sample index')\n",
    "plt.ylabel('Sample value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now initialise, fit, and make a prediction for an ARMA model in order to predict the next value in the timeseries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fit ARMA model\n",
    "model = ARIMA(data2, order=(2, 0, 1)) # p=2, q=1\n",
    "model_fit = model.fit()\n",
    "\n",
    "# Make prediction\n",
    "yhat = model_fit.predict(len(data2), len(data2)+3) # arguments denote which dataset indices to predict\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the MA model, for the ARMA model we are using the ARIMA() function, which is a more general function that goes beyod an ARMA model. By setting the second argument for 'order' to 0, we can achieve an ARMA function. There are many more input arguments that we can use in the ARIMA function; for more information on input/output arguments please view the relevant [ARIMA function documentation](https://www.statsmodels.org/stable/generated/statsmodels.tsa.arima.model.ARIMA.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Autoregressive Integrated Moving Average (ARIMA) Models\n",
    "\n",
    "In many cases, nonstationary data can be addressed by combining differencing with the autoregressive moving average model. This results in the **autoregressive integrated moving average model (ARIMA)**. In principle, differences of any order may be used, although first and second-order differences are most commonly used. If the order of the differencing is $d$, then this model is referred to as the ARIMA($p$, $d$, $q$) model.\n",
    "\n",
    "We will now use the first synthetic dataset of 100 samples to fit and make a prediction using an ARIMA(1,1,1) model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fit ARIMA model\n",
    "model = ARIMA(data, order=(1, 2, 1)) # p=1, d=2, q=1\n",
    "model_fit = model.fit()\n",
    "\n",
    "# Make prediction\n",
    "yhat = model_fit.predict(len(data), len(data)+3, typ='levels')\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Mining timeseries data - Discrete Fourier Transform (DFT)\n",
    "\n",
    "The discrete Fourier transform (DFT) is a commonly used method for data transformation in timeseries. More broadly, DFT is one of the fundamental tools used in the field of signal processing and is useful across numerous applications. The DFT is able to decompose a given timeseries into a linear combination of sinusoids.\n",
    "\n",
    "As an example, let us consider the following example timeseries $x$ which corresponds to a periodic timeseries (specifically a cosine function) perturbed by random noise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example synthetic timeseries\n",
    "N = 128\n",
    "n = np.arange(N)\n",
    "k = 10\n",
    "x = np.cos(2 * np.pi * (k * n / N) + 2 * (np.random.rand(N) - 0.5)) \n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.title('$x$')\n",
    "plt.plot(x, 'k')\n",
    "plt.xlabel('Time (index $n$)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DFT is efficiently implemented in python using the **Fast Fourier Transform (FFT)**, an algorithm that computes the discrete Fourier transform in a computationally efficient manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# numpy implementation of FFT\n",
    "Xfft = np.fft.fft(x)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.title('$|X|$ - magnitude of DFT')\n",
    "plt.plot(np.abs(Xfft), 'k')\n",
    "plt.xlabel('Frequency (index $k$)')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above figure is symmetric, and the upper half of the above figure can be discarded for analysing the timeseries, but it is useful if we would like to reconstruct the original timeseries. By inspecting the lower half of the DFT, we can see one clear peak. This peak indicates the predominant (i.e. most important) frequency of the timeseries. This frequency indicates the predominant period of the timeseries, in other words the length of the unit in samples which is repeated throughout the timeseries. We observe one such predominant frequency and corresponding period, since the timeseries consists of a (periodic) cosine function.\n",
    "\n",
    "If we have a timeseries which is expressed in a physical unit (e.g. seconds, milliseconds) it is possible to associate this predominant frequency to a frequency unit such as Hz; this is outside the scope of this lab tutorial.\n",
    "\n",
    "Finally, one of the benefits of the DFT and FFT is the fact that an inverse transform is also defined, which allows us to convert the above representation over frequency back to the original representation over time. This can be achieved by the **inverse DFT** (and the inverse FFT) as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Inverse FFT\n",
    "x_invfft = np.fft.ifft(Xfft)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.title('$x$')\n",
    "plt.plot(x_invfft, 'k')\n",
    "plt.xlabel('Time (index $n$)')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = 'maroon'>Assignment</font>\n",
    "\n",
    "Questions 1-3 are pen-and-paper exercises; questions 4-6 are coding exercises. For all your answers please show your workings (equations or code when applicable).\n",
    "\n",
    "1. Consider the following sentences related to data mining theory, and assume that each of the below sentences corresponds to a document $d$:\n",
    "    * Data refers to characteristics that are collected through observation.\n",
    "    * A dataset can be viewed as a collection of objects.\n",
    "    * Data objects are described by a number of attributes.\n",
    "    * An attribute is a characteristic or feature of an object.\n",
    "\n",
    "  1. Construct and display the document-term matrix for the above documents. Remove all stop words (here consider as stop words: articles, prepositions, conjunctions, pronouns, and common verbs) and punctuation marks; convert any plural nouns/adjectives to their singular form; and convert verbs to the present tense and first person singular form, before you construct the matrix. [1 mark out of 5]\n",
    "  2. Using the above constructed document-term matrix, calculate the inverse document frequency $\\mathit{idf}(w)$ for all words $w$ you have identified from question 1(a). [0.5 marks out of 5]\n",
    "  \n",
    "2. Consider a timeseries $y$ were we have obtained values of the timeseries for the following times $t$, as shown in the below table. Using linear interpolation, calculate the values $y$ of the timeseries for times $t = 3$ and $t=5$. [0.5 marks out of 5]\n",
    "\n",
    "| $t$  | 1  | 4  | 6  |\n",
    "|-|-|-|-|\n",
    "| $y$  | 2  | 8  | 5  |\n",
    "\n",
    "3. Consider the following timeseries $y = \\{0.1, 0.15, 0.2, 0.2, 0.3, 0.4, 0.25, 0.6, 0.5\\}$. Perform binning on the timeseries using $k=3$ values per bin, and show the resulting timeseries after binning. [0.5 marks out of 5]\n",
    "\n",
    "4. Load CSV file \"timeseries.csv\", which contains a univariate timeseries. Once loaded, convert the timeseries into a numpy array and use the numpy flatten() function to ensure that the loaded timeseries is one-dimensional. Compute the Discrete Fourier Transform (DFT) of the timeseries, and display plots for both the original timeseries and the magnitude of its DFT. How many predominant frequency components does the timeseries have? [1 mark out of 5]\n",
    "\n",
    "5. Using the daily births dataset from this lab tutorial, smooth the timeseries using trailing moving average smoothing and a window size that corresponds to one week; then replace any NaN values with zeros. Perform timeseries forecasting using the smoothed dataset in order to predict daily births for the first 5 days of 1960, using the models below. Show your forecasting results. [0.75 marks out of 5]\n",
    "    * AR model with $p=2$\n",
    "    * ARMA model with $p=2$ and $q=2$\n",
    "\n",
    "6. Using a similar process used in section 1 of this lab notebook, perform document clustering using k-means on the following wikipedia articles: supervised learning, unsupervised learning, semi-supervised learning, association rule learning, anomaly detection, cluster analysis, dimensionality reduction, regression analysis, statistical classification, data mining, data warehouse, online analytical processing. As with section 1, use the elbow metric to find an appropriate number of clusters. Discuss and display the document clustering results. [0.75 marks out of 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
